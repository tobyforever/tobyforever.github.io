<!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>大数据第3课 SparkSQL | Hexo</title><meta name="description" content="What?spark是统一的内存计算引擎。SparkSQL是通过SQL界面写spark任务程序。SparkStreaming是通过微批次的方式，实现实时流数据处理的效果。  流：在流的观点下，数据被看成是无界的，无限延伸，无穷无尽；流处理是搭建一个“管道”，数据随时进来随时处理，而这个“管道”程序是一直保持运行状态的。而传统观点下数据是有界的，类似一个数组，可以遍历。批处理是把数据一批一批地处理，"><meta property="og:type" content="article"><meta property="og:title" content="大数据第3课 SparkSQL"><meta property="og:url" content="https://www.linmingzhe.com/blog/2023/02/22/bigdata/bigdata003-sparksql/index.html"><meta property="og:site_name" content="z的个人博客"><meta property="og:description" content="What?spark是统一的内存计算引擎。SparkSQL是通过SQL界面写spark任务程序。SparkStreaming是通过微批次的方式，实现实时流数据处理的效果。  流：在流的观点下，数据被看成是无界的，无限延伸，无穷无尽；流处理是搭建一个“管道”，数据随时进来随时处理，而这个“管道”程序是一直保持运行状态的。而传统观点下数据是有界的，类似一个数组，可以遍历。批处理是把数据一批一批地处理，"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/tobyforever/uploadpic/upload/20230307173046.png"><meta property="article:published_time" content="2023-02-22T12:43:51.000Z"><meta property="article:modified_time" content="2023-06-06T05:14:51.713Z"><meta property="article:author" content="z"><meta property="article:tag" content="大数据"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/tobyforever/uploadpic/upload/20230307173046.png"><link rel="canonical" href="https://www.linmingzhe.com/blog/2023/02/22/bigdata/bigdata003-sparksql/index.html"><link rel="alternate" href="/atom.xml" title="z的个人博客" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 6.3.0"></head><body class="main-center theme-blue" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://www.linmingzhe.com" target="_blank"><img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">z</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">keep walking, keep thinking</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> China</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="搜索"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">首页</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">归档</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">分类</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">标签</span></a></li><li class="menu-item menu-item-repository"><a href="/repository"><i class="icon icon-project"></i> <span class="menu-title">项目</span></a></li><li class="menu-item menu-item-books"><a href="/books"><i class="icon icon-book-fill"></i> <span class="menu-title">书单</span></a></li><li class="menu-item menu-item-links"><a href="/links"><i class="icon icon-friendship"></i> <span class="menu-title">友链</span></a></li><li class="menu-item menu-item-about"><a href="/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">关于</span></a></li></ul><ul class="social-links"><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">公告</h3><div class="widget-body"><div id="board"><div class="content"><p>欢迎交流与分享经验!</p></div></div></div></div><div class="widget"><h3 class="widget-title">标签</h3><div class="widget-body"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/bigdata/" rel="tag">bigdata</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gis/" rel="tag">gis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/js/" rel="tag">js</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/liberal-education/" rel="tag">liberal-education</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/network/" rel="tag">network</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E8%82%B2/" rel="tag">教育</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag">数学</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%94%E8%AE%B2/" rel="tag">演讲</a><span class="tag-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签云</h3><div class="widget-body tagcloud"><a href="/tags/bigdata/" style="font-size:13px">bigdata</a> <a href="/tags/gis/" style="font-size:13px">gis</a> <a href="/tags/java/" style="font-size:13px">java</a> <a href="/tags/js/" style="font-size:13px">js</a> <a href="/tags/liberal-education/" style="font-size:13px">liberal-education</a> <a href="/tags/linux/" style="font-size:13.5px">linux</a> <a href="/tags/network/" style="font-size:13.5px">network</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size:13.75px">大数据</a> <a href="/tags/%E6%95%99%E8%82%B2/" style="font-size:14px">教育</a> <a href="/tags/%E6%95%B0%E5%AD%A6/" style="font-size:13.25px">数学</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size:13.25px">数据库</a> <a href="/tags/%E6%BC%94%E8%AE%B2/" style="font-size:13px">演讲</a></div></div><div class="widget"><h3 class="widget-title">最新文章</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/blog/2023/08/15/education/writer-of-openresty/" class="title">从抄书到开源之巅：章亦春的程序人生</a></p><p class="item-date"><time datetime="2023-08-15T15:46:51.000Z" itemprop="datePublished">2023-08-15</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/blog/2023/08/07/education/function-plot-tool/" class="title">在线绘制函数图像</a></p><p class="item-date"><time datetime="2023-08-07T15:46:51.000Z" itemprop="datePublished">2023-08-07</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/blog/2023/08/06/education/top-ten-ted-talks/" class="title">十大必看TED演讲</a></p><p class="item-date"><time datetime="2023-08-06T15:46:51.000Z" itemprop="datePublished">2023-08-06</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/blog/2023/08/01/education/dont-work-join-a-cult/" class="title">不要工作</a></p><p class="item-date"><time datetime="2023-08-01T15:46:51.000Z" itemprop="datePublished">2023-08-01</time></p></div></li><li><div class="item-inner"><p class="item-category"></p><p class="item-title"><a href="/blog/2023/07/22/math/pass-ball/" class="title">传球法解决排列组合问题</a></p><p class="item-date"><time datetime="2023-07-22T15:46:51.000Z" itemprop="datePublished">2023-07-22</time></p></div></li></ul></div></div></div></aside><main class="main" role="main"><div class="content"><article id="post-bigdata/bigdata003-sparksql" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">大数据第3课 SparkSQL</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/blog/2023/02/22/bigdata/bigdata003-sparksql/" class="article-date"><time datetime="2023-02-22T12:43:51.000Z" itemprop="datePublished">2023-02-22</time> </a></span><span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link-link" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a> </span><span class="article-read hidden-xs"><i class="icon icon-eye-fill" aria-hidden="true"></i> <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span> </span></span><span class="post-comment"><i class="icon icon-comment"></i> <a href="/blog/2023/02/22/bigdata/bigdata003-sparksql/#comments" class="article-comment-link">评论</a></span></div></div><div class="article-entry marked-body" itemprop="articleBody"><h1 id="What"><a href="#What" class="headerlink" title="What?"></a>What?</h1><p>spark是统一的内存计算引擎。SparkSQL是通过SQL界面写spark任务程序。SparkStreaming是通过微批次的方式，实现实时流数据处理的效果。</p><blockquote><p>流：在流的观点下，数据被看成是无界的，无限延伸，无穷无尽；流处理是搭建一个“管道”，数据随时进来随时处理，而这个“管道”程序是一直保持运行状态的。<br>而传统观点下数据是有界的，类似一个数组，可以遍历。批处理是把数据一批一批地处理，数据处理完一批程序就结束。</p></blockquote><h1 id="Why"><a href="#Why" class="headerlink" title="Why?"></a>Why?</h1><p>(1)Spark把中间数据放在内存中，迭代运算效率高。mapreduce中的计算结果保存在磁盘上，而spark支持DAG图的分布式并行计算的编程框架，减少了迭代过程中数据的落地，提高了处理效率。<br>spark可以支持读写hive，从而避免hive执行mapreduce慢的问题。</p><p>(2)spark容错性高。引进了RDD,如果数据集一部分丢失，则可以重建。另外，在RDD计算时可以通过checkpoint来实现容错。</p><p>(3)spark更加通用。不像hadoop只提供map和reduce两种操作。spark提供的数据集操作类型有很多种，大致分为转换操作和行动操作。转换操作包括map,filter,flatmap,sample,groupbykey,reducebykey,union,join,cogroup,mapvalues,sort和partionby等多种操作类型，行动操作包括collect,reduce,lookup和save等操作类型。另外，各个处理节点之间的通信模型不再像Hadoop只有shuffle一种模式，用户可以命名，物化，控制中间结果的存储，分区等。</p><h1 id="How"><a href="#How" class="headerlink" title="How?"></a>How?</h1><h2 id="spark的数据模型"><a href="#spark的数据模型" class="headerlink" title="spark的数据模型"></a>spark的数据模型</h2><p>截止到目前（2023）， spark最新的版本号是3.3.2<br>spark的数据模型从旧到新（或者从低级到高级）分别为RDD、DataSet(spark1.6开始)、<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes">DataFrame</a>。</p><p>spark的dataset&#x2F;dataframe是有列名的列，类似于关系数据库的table，或者大数据分布式版本的python pandas的dataframe。ds&#x2F;df在java&#x2F;scala语言中支持强类型，并且提供了常见的函数式算子如flatMap等用于对数据进行函数式变换(transform)操作。通过使用dataset&#x2F;dataframe，可以强类型地访问数据表的单元，以及进行transform。在scala、java中，dataframe实际上是Dataset[Row]。</p><p>为什么要通过RDD和函数式进行数据操作？因为大数据场景下移动数据的成本很高，需要移动计算程序并且把计算程序并行化，而函数式的immutable性质正好满足了这类场景。</p><p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations">RDD的操作</a>分为两类，transform和action。像flatMap、filter这种算子就属于transform，也就是从一个RDD转换为另一个RDD，程序调用的时候并不会立即执行（tranforms are lazy）；像first、take、count、collect这种就属于action，会结束数据处理程序并返回计算结果。</p><h2 id="SparkSQL的处理流程"><a href="#SparkSQL的处理流程" class="headerlink" title="SparkSQL的处理流程"></a>SparkSQL的处理流程</h2><h3 id="获取sparkcontext"><a href="#获取sparkcontext" class="headerlink" title="获取sparkcontext"></a>获取sparkcontext</h3><p>由于程序并非单机运行，实际执行环境可能是在多台机器上，因此spark把执行环境抽象为了sparkcontext，需要先获取sparkcontext或者sparksession对象sc，然后再调用sc.sql()方法进行sql查询。</p><blockquote><p>在zeppelin notebook环境中编写脚本时，sparkcontext已经被注入为sc对象，可以直接引用。</p></blockquote><h3 id="从数据源读取数据"><a href="#从数据源读取数据" class="headerlink" title="从数据源读取数据"></a>从数据源读取数据</h3><p>常见的数据源包括：</p><h4 id="hive表"><a href="#hive表" class="headerlink" title="hive表"></a>hive表</h4><p>spark可以通过sql直接对hive进行操作 <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html">官网文档</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">key: <span class="type">Int</span>, value: <span class="type">String</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// warehouseLocation points to the default location for managed databases and tables</span></span><br><span class="line"><span class="keyword">val</span> warehouseLocation = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;spark-warehouse&quot;</span>).getAbsolutePath</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">&quot;Spark Hive Example&quot;</span>)</span><br><span class="line">  .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, warehouseLocation)</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.sql</span><br><span class="line"></span><br><span class="line">sql(<span class="string">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span>)</span><br><span class="line">sql(<span class="string">&quot;LOAD DATA LOCAL INPATH &#x27;examples/src/main/resources/kv1.txt&#x27; INTO TABLE src&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries are expressed in HiveQL</span></span><br><span class="line">sql(<span class="string">&quot;SELECT * FROM src&quot;</span>).show()</span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |key|  value|</span></span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |238|val_238|</span></span><br><span class="line"><span class="comment">// | 86| val_86|</span></span><br><span class="line"><span class="comment">// |311|val_311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Aggregation queries are also supported.</span></span><br><span class="line">sql(<span class="string">&quot;SELECT COUNT(*) FROM src&quot;</span>).show()</span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |count(1)|</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |    500 |</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span></span><br><span class="line"><span class="keyword">val</span> sqlDF = sql(<span class="string">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span></span><br><span class="line"><span class="keyword">val</span> stringsDS = sqlDF.map &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Row</span>(key: <span class="type">Int</span>, value: <span class="type">String</span>) =&gt; <span class="string">s&quot;Key: <span class="subst">$key</span>, Value: <span class="subst">$value</span>&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |               value|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// You can also use DataFrames to create temporary views within a SparkSession.</span></span><br><span class="line"><span class="keyword">val</span> recordsDF = spark.createDataFrame((<span class="number">1</span> to <span class="number">100</span>).map(i =&gt; <span class="type">Record</span>(i, <span class="string">s&quot;val_<span class="subst">$i</span>&quot;</span>)))</span><br><span class="line">recordsDF.createOrReplaceTempView(<span class="string">&quot;records&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries can then join DataFrame data with data stored in Hive.</span></span><br><span class="line">sql(<span class="string">&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;</span>).show()</span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |key| value|key| value|</span></span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></span><br><span class="line"><span class="comment">// |  4| val_4|  4| val_4|</span></span><br><span class="line"><span class="comment">// |  5| val_5|  5| val_5|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax</span></span><br><span class="line"><span class="comment">// `USING hive`</span></span><br><span class="line">sql(<span class="string">&quot;CREATE TABLE hive_records(key int, value string) STORED AS PARQUET&quot;</span>)</span><br><span class="line"><span class="comment">// Save DataFrame to the Hive managed table</span></span><br><span class="line"><span class="keyword">val</span> df = spark.table(<span class="string">&quot;src&quot;</span>)</span><br><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">&quot;hive_records&quot;</span>)</span><br><span class="line"><span class="comment">// After insertion, the Hive managed table has data now</span></span><br><span class="line">sql(<span class="string">&quot;SELECT * FROM hive_records&quot;</span>).show()</span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |key|  value|</span></span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |238|val_238|</span></span><br><span class="line"><span class="comment">// | 86| val_86|</span></span><br><span class="line"><span class="comment">// |311|val_311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Prepare a Parquet data directory</span></span><br><span class="line"><span class="keyword">val</span> dataDir = <span class="string">&quot;/tmp/parquet_data&quot;</span></span><br><span class="line">spark.range(<span class="number">10</span>).write.parquet(dataDir)</span><br><span class="line"><span class="comment">// Create a Hive external Parquet table</span></span><br><span class="line">sql(<span class="string">s&quot;CREATE EXTERNAL TABLE hive_bigints(id bigint) STORED AS PARQUET LOCATION &#x27;<span class="subst">$dataDir</span>&#x27;&quot;</span>)</span><br><span class="line"><span class="comment">// The Hive external table should already have data</span></span><br><span class="line">sql(<span class="string">&quot;SELECT * FROM hive_bigints&quot;</span>).show()</span><br><span class="line"><span class="comment">// +---+</span></span><br><span class="line"><span class="comment">// | id|</span></span><br><span class="line"><span class="comment">// +---+</span></span><br><span class="line"><span class="comment">// |  0|</span></span><br><span class="line"><span class="comment">// |  1|</span></span><br><span class="line"><span class="comment">// |  2|</span></span><br><span class="line"><span class="comment">// ... Order may vary, as spark processes the partitions in parallel.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Turn on flag for Hive Dynamic Partitioning</span></span><br><span class="line">spark.sqlContext.setConf(<span class="string">&quot;hive.exec.dynamic.partition&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">spark.sqlContext.setConf(<span class="string">&quot;hive.exec.dynamic.partition.mode&quot;</span>, <span class="string">&quot;nonstrict&quot;</span>)</span><br><span class="line"><span class="comment">// Create a Hive partitioned table using DataFrame API</span></span><br><span class="line">df.write.partitionBy(<span class="string">&quot;key&quot;</span>).format(<span class="string">&quot;hive&quot;</span>).saveAsTable(<span class="string">&quot;hive_part_tbl&quot;</span>)</span><br><span class="line"><span class="comment">// Partitioned column `key` will be moved to the end of the schema.</span></span><br><span class="line">sql(<span class="string">&quot;SELECT * FROM hive_part_tbl&quot;</span>).show()</span><br><span class="line"><span class="comment">// +-------+---+</span></span><br><span class="line"><span class="comment">// |  value|key|</span></span><br><span class="line"><span class="comment">// +-------+---+</span></span><br><span class="line"><span class="comment">// |val_238|238|</span></span><br><span class="line"><span class="comment">// | val_86| 86|</span></span><br><span class="line"><span class="comment">// |val_311|311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure><h4 id="txt"><a href="#txt" class="headerlink" title="txt"></a>txt</h4><p>参见<a target="_blank" rel="noopener" href="https://github.com/tobyforever/geektime-bigdata-homework/blob/main/sparkwithscala/src/main/scala/Main.scala">作业</a>,<br>注意txt不如csv方便，csv文件里可以设置表头行直接解析成dataframe，而txt因为没有列名信息需要手动做schema mapping。</p><h4 id="csv"><a href="#csv" class="headerlink" title="csv"></a>csv</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleDFCsv = spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;examples/src/main/resources/people.csv&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="json"><a href="#json" class="headerlink" title="json"></a>json</h4><p>和csv类似，属于带自带schema的数据。</p><h4 id="parquet"><a href="#parquet" class="headerlink" title="parquet"></a>parquet</h4><p>Apache Parquet是Cloudera和twitter 2013年推出的Hadoop生态圈的一种新型列式存储二进制格式，针对大数据、写一次读多次（WORM）场景做了优化，在数据压缩性和查询性能（谓词下推、投影下推）方面有优势。自带元数据 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/helongBlog/p/13750315.html">参考</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">usersDF.write.format(<span class="string">&quot;parquet&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;parquet.bloom.filter.enabled#favorite_color&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;parquet.bloom.filter.expected.ndv#favorite_color&quot;</span>, <span class="string">&quot;1000000&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;parquet.enable.dictionary&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;parquet.page.write-checksum.enabled&quot;</span>, <span class="string">&quot;false&quot;</span>)</span><br><span class="line">  .save(<span class="string">&quot;users_with_options.parquet&quot;</span>)</span><br></pre></td></tr></table></figure><p>直接在sql语句中连接数据文件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val sqlDF <span class="operator">=</span> spark.sql(&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;)</span><br></pre></td></tr></table></figure><h4 id="avro"><a href="#avro" class="headerlink" title="avro"></a>avro</h4><p>avro是hadoop在2009年推出的行式存储格式，文件里schema用json而数据用二进制。</p><h4 id="各个数据文件格式的比较："><a href="#各个数据文件格式的比较：" class="headerlink" title="各个数据文件格式的比较："></a>各个数据文件格式的比较：</h4><p><img src="https://cdn.jsdelivr.net/gh/tobyforever/uploadpic/upload/20230307173046.png"></p><h3 id="对数据进行查询"><a href="#对数据进行查询" class="headerlink" title="对数据进行查询"></a>对数据进行查询</h3><p>查询可以通过2种方式：<br>1、dataframe api</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ans = peopleDF.filter(<span class="string">&quot;sex=\&quot;男\&quot;&quot;</span>).count()</span><br><span class="line">  <span class="type">System</span>.out.println(<span class="string">&quot;一共有多个男生参加考试？&quot;</span> + ans)</span><br></pre></td></tr></table></figure><p>2、sql查询</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> ans3 = peopleDF.sqlContext.sql(<span class="string">&quot;select avg(score) from people where name in ( select name from  people group by name having sum(score)&gt;150 and age &gt;=19) and name not in (select distinct name from people where subject=\&quot;math\&quot; and score &lt; 70)&quot;</span>);</span><br><span class="line">    <span class="type">System</span>.out.println(<span class="string">&quot;总成绩大于150分，且数学大于等于70，且年龄大于等于19岁的学生的平均成绩是多少？&quot;</span> + ans3.first().get(<span class="number">0</span>));</span><br></pre></td></tr></table></figure></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="https://www.linmingzhe.com/blog/2023/02/22/bigdata/bigdata003-sparksql/" title="大数据第3课 SparkSQL" target="_blank" rel="external">https://www.linmingzhe.com/blog/2023/02/22/bigdata/bigdata003-sparksql/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！</li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://www.linmingzhe.com" target="_blank" class="img-burn thumb-sm visible-lg"><img src="/images/avatar.jpg" class="img-rounded w-full" alt=""></a></div><div class="media-body"><h3 class="media-heading"><a href="https://www.linmingzhe.com" target="_blank"><span class="text-dark">z</span><small class="ml-1x">keep walking, keep thinking</small></a></h3><div>不积跬步无以至千里</div></div></figure></div></div></div></article><section id="comments"><div id="vcomments"></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="/blog/2023/02/24/bigdata/bigdata004-iot/" title="大数据第4课 物联网大数据组件选型"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a></li><li class="next"><a href="/blog/2023/02/21/bigdata/bigdata002-hive/" title="大数据第2课 hive"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li></ul><div class="bar-right"><div class="share-component" data-sites="wechat" data-mobile-sites="wechat"></div></div></div></nav></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.</div></div></footer><script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>window.INSIGHT_CONFIG={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(未命名)"},ROOT_URL:"/",CONTENT_URL:"/content.json"}</script><script src="/js/insight.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/npm/valine"></script><script type="text/javascript">var GUEST=["nick","mail","link"],meta=(meta="nick,mail,link").split(",").filter(function(e){return-1<GUEST.indexOf(e)});new Valine({el:"#vcomments",verify:!1,notify:!1,appId:"",appKey:"",placeholder:"Just go go",avatar:"mm",meta:meta,pageSize:"10",visitor:!1})</script></body></html>